{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate dataset of fiction stories from Mistral-7B-Instruct (baseline model) for classifier training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to run in colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to install the necessary requirements if you are running in colab\n",
    "# !pip install -U simpletransformers\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q trl xformers wandb datasets einops gradio sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the notebook\n",
    "import torch\n",
    "import simpletransformers\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, platform, gradio, warnings\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "dir_root = './' # comment this line if you are running in colab\n",
    "# dir_root = './drive/MyDrive/DL-ENS' # uncomment this line if you are running in colab\n",
    "dir_data = f'{dir_root}/dataset'\n",
    "list_to_generate_path = f'{dir_data}/story_prompts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model(Mistral 7B-Instruct)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the input in the expected form of the prompt\n",
    "def tokenize(tokenizer, text):\n",
    "  \"\"\"\n",
    "  Tokenize the input in the expected form of the prompt\n",
    "  inputs:\n",
    "    tokenizer: the tokenizer to use\n",
    "    text: the text to tokenize (before addapting the prompt)\n",
    "  outputs:\n",
    "    the tokenized text\n",
    "  \"\"\"\n",
    "  return tokenizer(f\"<s>[INST]These are the first lines of a work of fiction. Continue it. {text} [/INST]\", return_tensors = \"pt\", add_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for texts generation\n",
    "def generate_texts(model, tokenizer, texts):\n",
    "  \"\"\"\n",
    "  Generate texts from the input texts\n",
    "  inputs:\n",
    "    model: the model to use for generation\n",
    "    tokenizer: the tokenizer to use\n",
    "    texts: the inputs for text generation\n",
    "  outputs:\n",
    "    the generated texts\n",
    "  \"\"\"\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  generated_texts = []\n",
    "  for input in tqdm(texts):\n",
    "    tokens = tokenize(tokenizer, input)\n",
    "    model_inputs = tokens.to(device)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=500, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    generated_texts.extend(decoded)\n",
    "    del model_inputs\n",
    "    del decoded\n",
    "    del generated_ids\n",
    "  return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input texts for fiction generation\n",
    "texts = []\n",
    "with open(list_to_generate_path, 'r+', encoding='utf-8') as fd:\n",
    "  texts = fd.readlines()\n",
    "texts = [text[:-1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts generation\n",
    "generated_texts = generate_texts(model, tokenizer, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the results\n",
    "def save_generated_texts(texts):\n",
    "      \"\"\"\n",
    "      Save the generated texts\n",
    "      inputs:\n",
    "        texts: the generated texts without the prompt specific tokens\n",
    "    \"\"\"\n",
    "    dict_generated_texts = {'texts': []}\n",
    "    # clean the generated texts\n",
    "    patt = r'\\[INST]|\\[\\/INST]|\\<s>|\\</s>|This are the first lines of a work of fiction. Continue it.'\n",
    "    clean_texts = [re.sub(patt, '', x) for x in generated_texts]\n",
    "    \n",
    "    for i in range(len(clean_texts)):\n",
    "        dict_generated_texts['texts'].append(clean_texts[i])\n",
    "    with open(dir_data + f\"BaseModelCompletionsToTrainClassifier/dataset_mistral7B_gen_texts.json\", 'w+') as fd:\n",
    "        json.dump(dict_generated_texts, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_generated_texts(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one example of generated text\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
